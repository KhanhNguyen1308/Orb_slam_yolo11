# X99 SLAM Server - IMPROVED VERSION

## ğŸ¯ CÃC Cáº¢I TIáº¾N CHÃNH

### 1. **SLAM Core vá»›i Pose Estimation** âœ…
ÄÃ£ thÃªm Ä‘áº§y Ä‘á»§ chá»©c nÄƒng SLAM:

#### TrÆ°á»›c (Code cÅ©):
```python
# Chá»‰ cÃ³ feature extraction vÃ  matching
matches = self.orb_extractor.match_features(desc_left, desc_right)
# âŒ KhÃ´ng cÃ³ pose estimation
# âŒ KhÃ´ng cÃ³ tracking
# âŒ KhÃ´ng biáº¿t robot á»Ÿ Ä‘Ã¢u
```

#### Sau (Code má»›i):
```python
# Äáº§y Ä‘á»§ SLAM pipeline
class StereoSLAMTracker:
    - âœ… Feature extraction (ORB)
    - âœ… Stereo triangulation (3D reconstruction)
    - âœ… PnP RANSAC (camera motion estimation)
    - âœ… Keyframe management
    - âœ… Map building
    - âœ… Pose tracking (x, y, theta)
```

### 2. **TÃ­ch há»£p Path Planning**
- Occupancy grid Ä‘Æ°á»£c cáº­p nháº­t tá»« SLAM map
- A* path planning
- Gá»­i Ä‘Æ°á»ng Ä‘i vá» Jetson Nano

### 3. **Persistent Map Building**
- Voxel-based 3D map
- 2D occupancy grid
- LÆ°u/load map

### 4. **Better Visualization**
- Tracking quality indicator
- Pose display (x, y, theta)
- FPS counter
- Map visualization

---

## ğŸ”§ CÃ€I Äáº¶T

### Dependencies
```bash
pip install numpy opencv-python torch ultralytics --break-system-packages
```

### File Structure
```
x99_slam_server_improved.py  # Server chÃ­nh (ÄÃƒ Cáº¢I TIáº¾N)
path_planning.py              # A* path planning
persistent_map.py             # Map building
stereo_depth_mapping_optimized.py  # Stereo depth
calibration_params.npz        # Camera calibration (optional)
```

---

## ğŸš€ Sá»¬ Dá»¤NG

### 1. Cháº¡y Server (Basic)
```bash
python x99_slam_server_improved.py
```

### 2. Cháº¡y vá»›i Calibration
```bash
python x99_slam_server_improved.py --calibration calibration_params.npz
```

### 3. Cháº¡y khÃ´ng YOLO (tiáº¿t kiá»‡m GPU)
```bash
python x99_slam_server_improved.py --no-yolo
```

### 4. Cháº¡y khÃ´ng Path Planning
```bash
python x99_slam_server_improved.py --no-planning
```

### 5. Custom Ports
```bash
python x99_slam_server_improved.py --left-port 9001 --right-port 9002
```

---

## âŒ¨ï¸ KEYBOARD COMMANDS

Khi chÆ°Æ¡ng trÃ¬nh Ä‘ang cháº¡y:

- **`q`** - Quit (thoÃ¡t)
- **`g`** - Set Goal vÃ  plan path
  - Nháº­p tá»a Ä‘á»™: `Goal X: 2.0`, `Goal Y: 1.5`
  - Server sáº½ tÃ­nh Ä‘Æ°á»ng Ä‘i vÃ  gá»­i vá» Jetson
- **`s`** - Save map (lÆ°u map hiá»‡n táº¡i)
- **`r`** - Reset SLAM (reset toÃ n bá»™)

---

## ğŸ“Š THÃ”NG TIN HIá»‚N THá»Š

### Main Window: "X99 SLAM - Tracking + Navigation"
```
Frame: 1234
Tracking: GOOD          # GOOD/LOST/INIT/POOR
Keyframes: 15
Map Points: 3421
Tracked: 87 pts         # Sá»‘ Ä‘iá»ƒm Ä‘ang track
Pose: (1.23, 0.45, 0.52)  # x(m), y(m), theta(rad)
FPS: 15.2
```

### Tracking Quality
- **INIT** - Khá»Ÿi táº¡o (frame Ä‘áº§u tiÃªn)
- **GOOD** - Tracking tá»‘t (â‰¥15 inliers)
- **LOST** - Máº¥t tracking (<15 inliers)
- **POOR** - Ãt features (<50 features)

### Path Planning Window (náº¿u enable)
- Hiá»ƒn thá»‹ occupancy grid
- ÄÆ°á»ng Ä‘i Ä‘Æ°á»£c plan (cyan)
- Vá»‹ trÃ­ robot (green)
- Obstacles (black)

### Persistent Map Window
- 2D top-down view
- Robot trajectory (green line)
- Occupied space (black)
- Free space (white)

---

## ğŸ” SO SÃNH CODE CÅ¨ VS Má»šI

### A. SLAM Tracking

#### Code CÅ©:
```python
def process_stereo_frames(self, frame_left, frame_right):
    # Chá»‰ extract features
    kp_left, desc_left = self.orb_extractor.extract_features(frame_left)
    kp_right, desc_right = self.orb_extractor.extract_features(frame_right)
    
    # Match
    matches = self.orb_extractor.match_features(desc_left, desc_right)
    
    # âŒ KHÃ”NG CÃ“:
    # - Pose estimation
    # - Camera tracking
    # - 3D reconstruction
    # - Map building
```

#### Code Má»›i:
```python
def process_stereo_frame(self, img_left, img_right):
    # 1. Extract features (giá»‘ng cÅ©)
    kp_left, desc_left = self.extract_features(img_left)
    kp_right, desc_right = self.extract_features(img_right)
    
    # 2. âœ… TRACKING (Má»šI!)
    if self.prev_desc is not None:
        # Match vá»›i frame trÆ°á»›c
        # PnP RANSAC Ä‘á»ƒ tÃ­nh camera motion
        tracking_success = self.estimate_camera_motion(kp_left, desc_left)
        # Cáº­p nháº­t current_pose
    
    # 3. âœ… 3D RECONSTRUCTION (Má»šI!)
    points_3d, kp_indices, descs = self.triangulate_stereo_points(
        kp_left, desc_left, kp_right, desc_right
    )
    
    # 4. âœ… MAP BUILDING (Má»šI!)
    self._update_map(points_3d, kp_left, kp_indices, descs)
    
    # 5. âœ… KEYFRAME MANAGEMENT (Má»šI!)
    if self._should_create_keyframe():
        self._add_keyframe(...)
    
    return self.current_pose, self.map_points, tracking_quality
```

### B. Pose Estimation Chi Tiáº¿t

```python
def estimate_camera_motion(self, kp_current, desc_current):
    """
    CORE IMPROVEMENT: Camera pose estimation
    """
    # 1. Match vá»›i frame trÆ°á»›c
    matches = self.match_features(self.prev_desc, desc_current)
    
    # 2. Táº¡o 3D-2D correspondences
    points_3d = []  # 3D points tá»« frame trÆ°á»›c
    points_2d = []  # 2D keypoints á»Ÿ frame hiá»‡n táº¡i
    
    for match in matches:
        if prev_kp_idx in self.prev_kp_to_3d:
            points_3d.append(self.prev_kp_to_3d[prev_kp_idx])
            points_2d.append(kp_current[curr_kp_idx].pt)
    
    # 3. âœ… PnP RANSAC - CRITICAL!
    success, rvec, tvec, inliers = cv2.solvePnPRansac(
        objectPoints=points_3d,
        imagePoints=points_2d,
        cameraMatrix=self.K,
        ...
    )
    
    # 4. Convert to transformation matrix
    R, _ = cv2.Rodrigues(rvec)
    T_motion = np.eye(4)
    T_motion[:3, :3] = R
    T_motion[:3, 3] = tvec
    
    # 5. âœ… UPDATE POSE
    self.current_pose = self.current_pose @ np.linalg.inv(T_motion)
    
    return True
```

### C. Triangulation

```python
def triangulate_stereo_points(self, kp_left, desc_left, kp_right, desc_right):
    """
    Táº¡o 3D points tá»« stereo pair
    """
    # Match left-right
    matches = self.match_features(desc_left, desc_right)
    
    points_3d = []
    for match in matches:
        pt_left = kp_left[match.queryIdx].pt
        pt_right = kp_right[match.trainIdx].pt
        
        # Epipolar constraint
        if abs(pt_left[1] - pt_right[1]) > 2.0:
            continue
        
        # Disparity
        disparity = pt_left[0] - pt_right[0]
        
        # âœ… Compute depth
        depth = (self.baseline * self.K[0, 0]) / disparity
        
        # âœ… Unproject to 3D
        x = (pt_left[0] - self.K[0, 2]) * depth / self.K[0, 0]
        y = (pt_left[1] - self.K[1, 2]) * depth / self.K[1, 1]
        z = depth
        
        points_3d.append([x, y, z])
    
    return points_3d
```

---

## ğŸ“ HIá»‚U SLAM PIPELINE

### Flow Chart
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stereo Images  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature Extract â”‚  (ORB)
â”‚  Left + Right   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
    â–¼         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Track  â”‚ â”‚Stereo  â”‚
â”‚Previousâ”‚ â”‚ Match  â”‚
â”‚ Frame  â”‚ â”‚        â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚          â”‚
    â–¼          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PnP   â”‚ â”‚Triang- â”‚
â”‚RANSAC  â”‚ â”‚ulate   â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
    â”‚          â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚Update    â”‚
   â”‚Pose + Mapâ”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚Keyframe? â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Output  â”‚
   â”‚ Pose +   â”‚
   â”‚   Map    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### CÃ¡c KhÃ¡i Niá»‡m

#### 1. **PnP (Perspective-n-Point)**
- TÃ­nh camera pose tá»« 3D-2D correspondences
- Input: 
  - 3D points trong world frame (tá»« frame trÆ°á»›c)
  - 2D keypoints trong image hiá»‡n táº¡i
- Output: Rotation (R) vÃ  Translation (t)

#### 2. **RANSAC**
- Loáº¡i bá» outliers (matches sai)
- TÃ¬m transformation tá»‘t nháº¥t
- Chá»‰ giá»¯ láº¡i inliers (matches Ä‘Ãºng)

#### 3. **Triangulation**
- TÃ­nh 3D point tá»« 2 views (stereo)
- DÃ¹ng disparity: `Z = (f * B) / d`
- f: focal length, B: baseline, d: disparity

#### 4. **Keyframe**
- Frame quan trá»ng Ä‘Æ°á»£c lÆ°u láº¡i
- DÃ¹ng cho loop closure vÃ  relocalization
- Táº¡o khi robot di chuyá»ƒn Ä‘á»§ xa (>30cm hoáº·c >11Â°)

---

## ğŸ› TROUBLESHOOTING

### 1. "Tracking LOST"
**NguyÃªn nhÃ¢n:**
- Ãt features (texture tháº¥p, tá»‘i, motion blur)
- Motion quÃ¡ nhanh
- Occlusion

**Giáº£i phÃ¡p:**
```bash
# TÄƒng sá»‘ features
# Trong code, line ~150:
self.orb = cv2.ORB_create(nfeatures=2500)  # TÄƒng tá»« 1500
```

### 2. Pose khÃ´ng chÃ­nh xÃ¡c
**NguyÃªn nhÃ¢n:**
- Calibration kÃ©m
- Baseline khÃ´ng Ä‘Ãºng

**Giáº£i phÃ¡p:**
```bash
# Re-calibrate stereo cameras
# Hoáº·c Ä‘iá»u chá»‰nh baseline:
python x99_slam_server_improved.py --baseline 0.12  # Äo chÃ­nh xÃ¡c
```

### 3. FPS tháº¥p
**Giáº£i phÃ¡p:**
```bash
# Disable YOLO
python x99_slam_server_improved.py --no-yolo

# Giáº£m resolution (trong sender)
# Hoáº·c giáº£m sá»‘ features
```

### 4. Map drift (trÃ´i)
**NguyÃªn nhÃ¢n:**
- KhÃ´ng cÃ³ loop closure
- Tracking quality kÃ©m

**Giáº£i phÃ¡p:**
- Cáº£i thiá»‡n lighting
- ThÃªm texture vÃ o mÃ´i trÆ°á»ng
- (Future) Implement loop closure

---

## ğŸ“ˆ PERFORMANCE

### Typical Performance (AMD MI50)
- **Feature Extraction**: 5-8ms
- **Stereo Matching**: 10-15ms
- **PnP RANSAC**: 5-10ms
- **YOLO Segmentation**: 30-50ms
- **Total FPS**: 15-25 FPS

### Optimization Tips
1. **Disable YOLO khi test**: +10-15 FPS
2. **Reduce features**: 1000 thay vÃ¬ 1500
3. **Downscale images**: 640x480 â†’ 320x240
4. **Skip frames**: Process má»—i 2 frames

---

## ğŸ“ NEXT STEPS

### Æ¯u tiÃªn cao (1-2 tuáº§n):
- [ ] Test vá»›i real robot
- [ ] Tune PnP RANSAC parameters
- [ ] Add relocalization
- [ ] Semantic filtering (dÃ¹ng YOLO Ä‘á»ƒ filter dynamic objects)

### Trung háº¡n (1 thÃ¡ng):
- [ ] Loop closure detection
- [ ] Pose graph optimization
- [ ] Bundle adjustment
- [ ] Multi-session mapping

### DÃ i háº¡n:
- [ ] Switch to ORB-SLAM3
- [ ] Object-level SLAM
- [ ] Deep learning features (SuperPoint)

---

## ğŸ”— INTEGRATION vá»›i Jetson

### Data Flow
```
X99 Server                    Jetson Nano
    â”‚                             â”‚
    â”œâ”€â–º TCP 9003: Pose â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Navigation Controller
    â”‚   {x, y, theta}             â”‚
    â”‚                             â”‚
    â”œâ”€â–º TCP 9003: Path â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚ Pure Pursuit
    â”‚   [(x1,y1), (x2,y2), ...]   â”‚
    â”‚                             â”‚
    â—„â”€â”€ TCP 9001/9002 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ Camera Streams
        (Images)                  â”‚
```

### Jetson Side (No changes needed)
Jetson code (`jetson_navigation.py`) Ä‘Ã£ sáºµn sÃ ng nháº­n:
- Pose updates tá»« X99
- Path commands tá»« X99

Chá»‰ cáº§n cháº¡y:
```bash
# On Jetson
python jetson_navigation.py
```

---

## â“ FAQ

**Q: Táº¡i sao cáº§n PnP RANSAC?**
A: Äá»ƒ biáº¿t camera/robot Ä‘Ã£ di chuyá»ƒn bao nhiÃªu. KhÃ´ng cÃ³ nÃ³ thÃ¬ khÃ´ng biáº¿t mÃ¬nh á»Ÿ Ä‘Ã¢u.

**Q: Tracking GOOD nhÆ°ng pose sai?**
A: Check calibration. Baseline vÃ  focal length pháº£i chÃ­nh xÃ¡c.

**Q: CÃ³ thá»ƒ dÃ¹ng khÃ´ng cÃ³ calibration file?**
A: CÃ³, nhÆ°ng kÃ©m chÃ­nh xÃ¡c. Code sáº½ dÃ¹ng default values.

**Q: YOLO lÃ m gÃ¬?**
A: Hiá»‡n táº¡i chá»‰ visualization. TÆ°Æ¡ng lai: filter dynamic objects, semantic mapping.

**Q: Keyframe Ä‘á»ƒ lÃ m gÃ¬?**
A: Loop closure (detect khi quay láº¡i chá»— cÅ©), relocalization (tÃ¬m láº¡i vá»‹ trÃ­ khi lost).

---

## ğŸ“š REFERENCES

- **ORB-SLAM2 Paper**: https://arxiv.org/abs/1610.06475
- **PnP Tutorial**: https://docs.opencv.org/4.x/d5/d1f/calib3d_solvePnP.html
- **Stereo Vision**: https://docs.opencv.org/4.x/dd/d53/tutorial_py_depthmap.html

---

## ğŸ™ CREDITS

Original code: Your X99 robot system
Improvements: SLAM core, pose estimation, path planning integration
Based on: ORB-SLAM concepts, OpenCV examples

---

**Happy SLAMming! ğŸ¤–ğŸ“**